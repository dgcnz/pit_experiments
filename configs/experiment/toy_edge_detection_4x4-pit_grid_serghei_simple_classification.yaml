# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: toy_edge_classification
  - override /model: pit_serghei_grid_classification
  - override /callbacks: [grokking] #, early_stopping]
  - override /trainer: default

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["toy_edge_classification"]

seed: 0

trainer:
  min_epochs: 2
  max_epochs: 400
  overfit_batches: 3

model:
  optimizer:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 1
    # weight_decay: 0.01
    # momentum: 0.9
  # scheduler:
  #   _target_: src.utils.schedulers.LinearWarmupScheduler
  #   _partial_: true
  #   warmup_steps: 10
  net:
    img_width: 4
    img_height: 4
    num_blocks: 4
    embed_dim: 16
    nhead: 4
    dim_feedforward: 128
    num_classes: 1
    is_simple_classification: true
  lr_scheduler_interval: "step"

data:
  batch_size: 64
  num_samples: 10000
  width: 4
  height: 4
