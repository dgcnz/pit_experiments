# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: toy_mirror_4x4
  - override /model: pit_serghei_grid
  - override /callbacks: [grokking, early_stopping]
  - override /trainer: default

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["toy_mirror"]

seed: 0

hydra:
  mode: "MULTIRUN"
  sweeper:
    params:
      model.optimizer.lr: 0.001, 0.01, 0.1, 1
      model.optimizer.weight_decay: 0, 1
      model.net.num_blocks: 2, 4
      model.net.embed_dim: 32, 256
      model.net.nhead: 2, 4
      model.net.dim_feedforward: 32, 256

trainer:
  min_epochs: 2
  max_epochs: 400

model:
  optimizer:
    # grokking paper config
    _target_: torch.optim.SGD
    _partial_: true
    lr: 0.3
    # weight_decay: 0.01

  # optimizer:
  #   # grokking paper config
  #   _target_: torch.optim.AdamW
  #   _partial_: true
  #   lr: 0.001
  #   # weight_decay: 1.0
  #   weight_decay: 1.0
  #   betas:
  #     - 0.9
  #     - 0.98
  scheduler:
    _target_: src.utils.schedulers.LinearWarmupScheduler
    _partial_: true
    warmup_steps: 10
  net:
    img_width: 4
    img_height: 4
    num_blocks: 10
    embed_dim: 64
    nhead: 8
    dim_feedforward: 256
  lr_scheduler_interval: "step"

data:
  batch_size: 64
# deactivate logger for small experiments
# logger:
#   wandb:
#     tags: ${tags}
#     group: "tests"
#     project: "pit-experiments"
#     entity: "iexai"
#   aim:
#     experiment: "pit-experiments/tests"
