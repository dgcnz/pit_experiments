# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: toy_edge_classification
  - override /model: mlp_classification
  - override /callbacks: [grokking] #, early_stopping]
  - override /trainer: default

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["toy_edge_classification"]

seed: 0

trainer:
  min_epochs: 2
  max_epochs: 400
  overfit_batches: 3

model:
  optimizer:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 0.01
    # weight_decay: 10
    # momentum: 0.9
  scheduler:
    _target_: src.utils.schedulers.LinearWarmupScheduler
    _partial_: true
    warmup_steps: 10
  net:
    img_width: 4
    img_height: 4
    hidden_dim: 32
    num_layers: 2
    num_classes: 1
  lr_scheduler_interval: "step"

data:
  batch_size: 64
  num_samples: 10000
  width: 4
  height: 4
